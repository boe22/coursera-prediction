---
title: "Prediction Assignment Writeup"
author: "Remko Logemann"
date: "February 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(randomForest)
```

## Data preperation

Let us start by loading the training and testing datasets and view their dimensions.
```{r cars}
training <- read.csv('../pml-training.csv', na.strings = c("","NA"))
testing <- read.csv('../pml-testing.csv', na.strings = c("","NA"))
dim(training)
dim(testing)
```

So both the training and test set consist of 160 features. Next step is to clean the dataset, here we choose the simplest approach to just ignore all features with missing values. As the number of features is rather large this might work already sufficiently. Also we remove the first seven columns they have no predictive value. 

```{r}
training <- training[8:length(training)]
training <- training[, colSums(is.na(training)) == 0]

testing <- testing[8:length(testing)]
testing <- testing[, colSums(is.na(testing)) == 0]
```

## Partition data

Next we split the training set into a training (70%) and validation (30%) part in addition to the test set:
```{r}
PartData <- createDataPartition(training$classe, p=0.7, list=FALSE)
trainingSet <- training[PartData, ]
validationSet <- training[-PartData, ]
```

Next, let us check how the classe are divided in the trainingset: 
```{r}
plot(trainingSet$classe, main="Frequency of levels", xlab="classe", ylab="Frequency")
```
The figure above shows that the classe is distributed among the levels in the same order of magnetitude. Therefore no extra effort is required to balance the classe. 


## Model training and cross-validation
Next we train our random forest, where we use cross-validation with 5 folds to estimate the in and out-of sample errors later on:
```{r}
set.seed(1234)
rf <- train(classe ~ ., data = trainingSet, method = "rf", trControl=trainControl(method="cv", number=4))
```

```{r}
rf$finalModel
```


```{r}
print(rf)
```
Hence our in-sample-error is $1.03\%$. 

To estimate the out-of-sample error, we use the validation set which is untouched during the model training: 
```{r}
pred_rf <- predict(rf, validationSet)
confusionMatrix(pred_rf, validationSet$classe)
```
Therefore, our estimation of the out-of-sample error is __$0.58\%$__. 

Next, we apply the random forest model to the test set of 20: 
```{r}
predictTest <- predict(rf, testing)

predictTest
```

